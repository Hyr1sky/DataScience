# <center> 数据科学 实验二
何尉宁 2021213599 2021219106班
## 任务列表
**必做：**\
复现课件中线性SVM、决策树、朴素贝叶斯分类的示例，并相对课件代码作出如下作图修改
- [设定支持向量分类器的惩罚为0.05](#one1)
- [对朴素贝叶斯分类器的先验概率进行设定（可随机设定）](#one1)
- [在每张结果图上展示图例](#one3)
- [修改散点颜色为黄和绿](#one3)
  
_测试结果的正确率保留三位小数展示_

**选做：**\
自主选取其他的数据集，采用上述三类分类器进行分类，展示分类结果\
探究分类器的参数对于分类结果的影响并进行文字分析（选做）\
如：
- [DecisionTreeClassifier(max_depth=5)中max_depth设置对于结果的影响（如过拟合或者欠拟合）](#two1)
- [朴素贝叶斯分类器的先验概率修改对于分类的影响](#two2)
- [支持向量分类器不同核函数对于结果的影响](#two3)

_参数不限制于课件中代码所用到的参数，可以探究其他的参数_\
_其他分类方法的效果的对比分析（K近邻，随机森林等）_

<h2>1. 代码复现</h2>
<h4 id = "one1">1.1 修改惩罚系数与先验概率</h4>

1. Set the penalty to 0.5
2. Set the class prior to a random number

```python
names = ["Linear_SVM","Decision_Tree","Naive_Bayes"]
# 设置随机数用于先验概率
random_number = np.random.randint(0,100)
random_number /= 100
print(random_number)

classifiers = [
    SVC(kernel = "linear", C = 0.05),
    DecisionTreeClassifier(random_state = 44, max_depth = 5),
    GaussianNB(priors=[random_number, 1-random_number]),
]
```

<h4>1.2 随机噪声</h4>

1. RandomNoise
2. GaussianNoise

```python
X,y=make_classification(n_features=2,n_redundant=0,n_informative=2,
                        random_state=1,n_clusters_per_class=1)

random_num = np.random.RandomState(5) # 设置一个伪随机数种子
# 随机扰动噪声
RandomNoise = random_num.uniform(low = -1, high = 1, size = X.shape)
# 高斯噪声
GaussianNoise = random_num.normal(loc = 0, scale = 1, size = X.shape)

# X += 2*RandomNoise
X += 2*GaussianNoise

linearly_separable=(X,y) # 将上述得到的x，y够作为一个线性可分的数据集

datasets=[make_moons(noise=0.1,random_state=np.random.RandomState(5)),
         make_circles(noise=0.1,factor=0.5,random_state=1),
         linearly_separable
         ]
```

<h4 id = "one3"> 1.3 绘图 </h4>

修改了主题颜色与图案

![Alt text](image.png)

<h2>2. 选做</h2>

### 数据集介绍
#### Wine
> Donated on 6/30/1991\
> Using chemical analysis to determine the origin of wines

**Dataset Characteristics**
Tabular\
**Subject Area**
Physics and Chemistry\
**Associated Tasks**
Classification\
**Feature Type**
Integer, Real\
**Instances**
178\
**Features**
13

#### 2.2 数据处理

```python
wine_data = pd.DataFrame(wine_data)
wine_data.columns = ["Class","Alcohol","Malic acid","Ash","Alcalinity of ash","Magnesium","Total phenols",
                     "Flavanoids","Nonflavanoid phenols","Proanthocyanins","Color intensity","Hue",
                     "OD280/OD315 of diluted wines","Proline"]
wine_data.iloc[:,1:] = StandardScaler().fit_transform(wine_data.iloc[:,1:])
wine_data.head()
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Class</th>
      <th>Alcohol</th>
      <th>Malic acid</th>
      <th>Ash</th>
      <th>Alcalinity of ash</th>
      <th>Magnesium</th>
      <th>Total phenols</th>
      <th>Flavanoids</th>
      <th>Nonflavanoid phenols</th>
      <th>Proanthocyanins</th>
      <th>Color intensity</th>
      <th>Hue</th>
      <th>OD280/OD315 of diluted wines</th>
      <th>Proline</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>1.518613</td>
      <td>-0.562250</td>
      <td>0.232053</td>
      <td>-1.169593</td>
      <td>1.913905</td>
      <td>0.808997</td>
      <td>1.034819</td>
      <td>-0.659563</td>
      <td>1.224884</td>
      <td>0.251717</td>
      <td>0.362177</td>
      <td>1.847920</td>
      <td>1.013009</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>0.246290</td>
      <td>-0.499413</td>
      <td>-0.827996</td>
      <td>-2.490847</td>
      <td>0.018145</td>
      <td>0.568648</td>
      <td>0.733629</td>
      <td>-0.820719</td>
      <td>-0.544721</td>
      <td>-0.293321</td>
      <td>0.406051</td>
      <td>1.113449</td>
      <td>0.965242</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>0.196879</td>
      <td>0.021231</td>
      <td>1.109334</td>
      <td>-0.268738</td>
      <td>0.088358</td>
      <td>0.808997</td>
      <td>1.215533</td>
      <td>-0.498407</td>
      <td>2.135968</td>
      <td>0.269020</td>
      <td>0.318304</td>
      <td>0.788587</td>
      <td>1.395148</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.0</td>
      <td>1.691550</td>
      <td>-0.346811</td>
      <td>0.487926</td>
      <td>-0.809251</td>
      <td>0.930918</td>
      <td>2.491446</td>
      <td>1.466525</td>
      <td>-0.981875</td>
      <td>1.032155</td>
      <td>1.186068</td>
      <td>-0.427544</td>
      <td>1.184071</td>
      <td>2.334574</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.0</td>
      <td>0.295700</td>
      <td>0.227694</td>
      <td>1.840403</td>
      <td>0.451946</td>
      <td>1.281985</td>
      <td>0.808997</td>
      <td>0.663351</td>
      <td>0.226796</td>
      <td>0.401404</td>
      <td>-0.319276</td>
      <td>0.362177</td>
      <td>0.449601</td>
      <td>-0.037874</td>
    </tr>
  </tbody>
</table>
</div>

<h4 id = "two1"> 2.3 不同深度的决策树 </h4>

因为这次选择的`wine`数据集本身规模不大，所以在深度选择上只使用了1和10来进行对比。\
`1:` 明显欠拟合，score非常低\
`10:` 拟合很好，基本上已经到达score上限

`Max_Depth = 1`\
Decision Tree score:  0.625

![Alt text](image-3.png)

`Max_Depth = 10`\
Decision Tree score:  0.9444444444444444

![Alt text](image-1.png)

```python
# 使用决策树进行分类
X = wine_data.iloc[:,1:]
y = wine_data.iloc[:,0]
X_train, X_test, y_train, y_test = \
        train_test_split(X, y, test_size=0.4, random_state=42)
clf = DecisionTreeClassifier(random_state = 44, max_depth = 1)
clf.fit(X_train, y_train)
score = clf.score(X_test, y_test)
print("Decision Tree score: ", score)

figure = plt.figure(figsize=(36, 18))
i = 1
x_min, x_max = X_train.iloc[:,0].min() - 0.5, X_train.iloc[:,0].max() + 0.5
y_min, y_max = X_train.iloc[:,1].min() - 0.5, X_train.iloc[:,1].max() + 0.5
h = 0.02
xx, yy = np.meshgrid(
    np.linspace(X_train.iloc[:,0].min(), X_train.iloc[:,0].max(), 100),
    np.linspace(X_train.iloc[:,1].min(), X_train.iloc[:,1].max(), 100)
)
cm = ListedColormap((['yellow', 'green']))
cm_bright = ListedColormap(['#FFA500', '#008000'])

ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
ax.set_title("Input data")
ax.scatter(X_train.iloc[:,0], X_train.iloc[:,1], c=y_train, cmap=cm_bright,
           edgecolors='k', marker='o', label='train set')
ax.scatter(X_test.iloc[:,0], X_test.iloc[:,1], c=y_test, cmap=cm_bright, alpha=0.6,
           facecolor='k', marker='x', label='test set')
ax.set_xlim(xx.min(), xx.max())
ax.set_ylim(yy.min(), yy.max())
ax.set_xticks(())
ax.set_yticks(())
plt.legend()
```

<h4 id = "two2"> 2.4 朴素贝叶斯中不同先验概率的选择 </h4>

选取了`features`中相关系数最高的两项特征进行分类，选取`default`先验概率与`[0.2, 0.3, 0.5]`两种

`priors = defalut`\
Naive Bayes score:  0.7777777777777778

![Alt text](image-4.png)

`priors = [0.2, 0.3, 0.5]`\
Naive Bayes score:  0.8194444444444444

![Alt text](image-5.png)

```python
wine = np.loadtxt(file_path + "/wine.data", delimiter=",")
wine = pd.DataFrame(wine)
wine.columns = ["Class","Alcohol","Malic acid","Ash","Alcalinity of ash","Magnesium","Total phenols",
                        "Flavanoids","Nonflavanoid phenols","Proanthocyanins","Color intensity","Hue",
                        "OD280/OD315 of diluted wines","Proline"]

selected_features = ["Alcohol", "Malic acid"]
wine_subset = wine[["Class"] + selected_features]

X = wine_subset[selected_features]
y = wine_subset["Class"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

clf = GaussianNB(priors=[0.2, 0.3, 0.5])
clf.fit(X_train, y_train)
score = clf.score(X_test, y_test)
print("Naive Bayes score: ", score)

figure = plt.figure(figsize=(36, 18))
i = 1
x_min, x_max = X_train.iloc[:, 0].min() - 0.5, X_train.iloc[:, 0].max() + 0.5
y_min, y_max = X_train.iloc[:, 1].min() - 0.5, X_train.iloc[:, 1].max() + 0.5
h = 0.02
xx, yy = np.meshgrid(
    np.linspace(X_train.iloc[:, 0].min(), X_train.iloc[:, 0].max(), 100),
    np.linspace(X_train.iloc[:, 1].min(), X_train.iloc[:, 1].max(), 100)
)
cm = ListedColormap((['yellow', 'green']))
cm_bright = ListedColormap(['#FFA500', '#008000'])

ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
ax.set_title("Input data")
ax.scatter(X_train.iloc[:, 0], X_train.iloc[:, 1], c=y_train, cmap=cm_bright,
           edgecolors='k', marker='o', label='train set')
ax.scatter(X_test.iloc[:, 0], X_test.iloc[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,
           facecolor='k', marker='x', label='test set')
ax.set_xlim(xx.min(), xx.max())
ax.set_ylim(yy.min(), yy.max())
ax.set_xticks(())
ax.set_yticks(())
plt.legend()
```

<h4 id = "two3"> 2.5 SVC不同kernel选择 </h4>

分别使用了`linear`和`poly`核函数去拟合特征分布，但多项式的结果较为一般，因为这个简单分类问题基本上是线性的，多项式有点多此一举了。\
同时**需要修改惩罚系数，否则准确率会大幅下降，非常欠拟合**

`linear`\
SVC score:  0.9722222222222222

![Alt text](image-6.png)

`poly`\
SVC score:  0.9027777777777778

![Alt text](image-7.png)

```python
# 使用SVC进行分类
X = wine_data.iloc[:,1:]
y = wine_data.iloc[:,0]
X_train, X_test, y_train, y_test = \
        train_test_split(X, y, test_size=0.4, random_state=42)
clf = SVC(kernel = "poly", C = 1)
clf.fit(X_train, y_train)
score = clf.score(X_test, y_test)
print("SVC score: ", score)

figure = plt.figure(figsize=(36, 18))
i = 1
x_min, x_max = X_train.iloc[:,0].min() - 0.5, X_train.iloc[:,0].max() + 0.5
y_min, y_max = X_train.iloc[:,1].min() - 0.5, X_train.iloc[:,1].max() + 0.5
h = 0.02
xx, yy = np.meshgrid(
    np.linspace(X_train.iloc[:,0].min(), X_train.iloc[:,0].max(), 100),
    np.linspace(X_train.iloc[:,1].min(), X_train.iloc[:,1].max(), 100)
)
cm = ListedColormap((['yellow', 'green']))
cm_bright = ListedColormap(['#FFA500', '#008000'])

ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
ax.set_title("Input data")
ax.scatter(X_train.iloc[:,0], X_train.iloc[:,1], c=y_train, cmap=cm_bright,
           edgecolors='k', marker='o', label='train set')
ax.scatter(X_test.iloc[:,0], X_test.iloc[:,1], c=y_test, cmap=cm_bright, alpha=0.6,
           facecolor='k', marker='x', label='test set')
ax.set_xlim(xx.min(), xx.max())
ax.set_ylim(yy.min(), yy.max())
ax.set_xticks(())
ax.set_yticks(())
plt.legend()
```